{
  "guid": "https://old.reddit.com/r/FacebookAds/comments/1ptzy15/how_do_you_test_new_ads_in_meta_without_hurting/",
  "title": "<h1><a href=\"https://old.reddit.com/r/FacebookAds/comments/1ptzy15/how_do_you_test_new_ads_in_meta_without_hurting/\" target=\"_blank\">How do you test new ads in Meta without hurting winning ones?</a></h1>",
  "link": "https://old.reddit.com/r/FacebookAds/comments/1ptzy15/how_do_you_test_new_ads_in_meta_without_hurting/",
  "pubDate": "2025-12-23T17:36:26.000Z",
  "description": "<div><p>I’m curious how others are handling ad testing in Meta Ads, because we keep going back and forth on this internally.</p> <p>Right now, our main doubt is:</p> <p>Is it better to test new ads inside the same campaign/ad set as the winning ads, or to create a separate testing campaign/ad set?</p> <p>Some context from our experience:</p> <ul> <li>With low budgets (e.g. €20/day), creating a separate test campaign feels risky because impressions get too diluted.</li> <li>At the ad set level, you can somewhat force spend, but at the ad level, Meta doesn’t distribute impressions evenly. Ads get a bit of traffic and only scale if performance is good.</li> <li>Because of that, comparing variants cleanly is hard some ads barely get impressions before Meta decides.</li> <li>What we usually do is keep the best-performing ads live and gradually introduce new ones, pausing underperformers so impressions don’t get too fragmented.</li> <li>We track CTR, CPC and sometimes video retention to decide whether a “test” ad is promising, not just final CPA.</li> <li>Still, it feels very optimization-driven (“what works best right now”) rather than learning-driven (“how does each new ad actually perform”).</li> </ul> <p>One idea we’re considering:</p> <ul> <li>Keep a main campaign with proven winners.</li> <li>Use a dedicated test ad set or campaign with capped budget (e.g. 20% of spend), and migrate winners once they show signal.</li> <li>But we’re worried about internal competition between campaigns hurting overall performance.</li> </ul> <p>So I’d love to hear:</p> <ul> <li>How do you structure testing vs scaling?</li> <li>Do you test inside winning ad sets or isolate tests?</li> <li>How do you handle this differently at low vs high budgets?</li> <li>Any frameworks or rules you swear by?</li> </ul> <p>Thanks in advance genuinely interested in how others solve this.</p> </div>   submitted by   <a href=\"https://old.reddit.com/user/No-Internet-7697\"> /u/No-Internet-7697 </a> <br /> <span><a href=\"https://old.reddit.com/r/FacebookAds/comments/1ptzy15/how_do_you_test_new_ads_in_meta_without_hurting/\">[link]</a></span>   <span><a href=\"https://old.reddit.com/r/FacebookAds/comments/1ptzy15/how_do_you_test_new_ads_in_meta_without_hurting/\">[comments]</a></span>",
  "timestamp": 1766511386000,
  "source": "9b633bb65108",
  "category": ""
}